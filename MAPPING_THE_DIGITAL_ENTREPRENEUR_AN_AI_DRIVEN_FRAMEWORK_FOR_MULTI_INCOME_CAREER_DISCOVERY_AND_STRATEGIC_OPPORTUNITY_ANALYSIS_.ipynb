{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Library Install"
      ],
      "metadata": {
        "id": "ai6S5pHOtj-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install praw"
      ],
      "metadata": {
        "id": "XNLoBZLBhrQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install gensim"
      ],
      "metadata": {
        "id": "eJFvf_UeuxWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# pip install bertopic"
      ],
      "metadata": {
        "id": "llHv6waUvJCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library Download"
      ],
      "metadata": {
        "id": "7vnw7xW2tniD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code sets up the environment for text preprocessing and NLP tasks. It first defines Boolean flags (_HAS_SPACY, _HAS_SBERT, _HAS_BERTOPIC, _HAS_GENSIM) to indicate the availability of key libraries for NLP and topic\n",
        "modeling. It then imports NLTK and Gensim, along with specific modules for stopwords and lemmatization. NLTK datasets—punkt, stopwords, wordnet, omw-1.4, and punkt_tab—are downloaded to enable tokenization, stopword removal, and lemmatization. Finally, a set of English stopwords is created, and a WordNetLemmatizer object is initialized, providing essential tools for cleaning, normalizing, and preparing text data for advanced analysis such as embeddings or topic modeling."
      ],
      "metadata": {
        "id": "8mnE1yckiqpW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "_HAS_SPACY = True\n",
        "_HAS_SBERT = True\n",
        "_HAS_BERTOPIC = True\n",
        "_HAS_GENSIM = True\n",
        "\n",
        "import nltk\n",
        "import gensim\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "nltk.download(\"wordnet\", quiet=True)\n",
        "nltk.download(\"omw-1.4\", quiet=True)\n",
        "nltk.download(\"punkt_tab\", quiet=True)\n",
        "STOPWORDS = set(stopwords.words(\"english\"))\n",
        "LEMMATIZER = WordNetLemmatizer()"
      ],
      "metadata": {
        "id": "qJtKYhI-smuj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library Import"
      ],
      "metadata": {
        "id": "9m_tK50otrQj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code imports a wide range of libraries and modules required for text processing, NLP, topic modeling, clustering, and machine learning tasks. Core Python modules like os, re, json, and time handle file operations, regular expressions, JSON parsing, and timing operations. Libraries like nltk, spacy, gensim, praw, and SentenceTransformer support tokenization, lemmatization, embeddings, and Reddit data extraction. pandas and numpy manage and analyze data, while seaborn helps in visualization. BERTopic, Word2Vec, LDA, NMF, KMeans, and DBSCAN enable topic modeling and clustering. Scikit-learn modules provide tools for vectorization (TfidfVectorizer, CountVectorizer), model training (LogisticRegression), evaluation (classification_report, accuracy_score), and splitting data (train_test_split). Finally, warnings.filterwarnings(\"ignore\") suppresses unnecessary warning messages during execution, keeping outputs clean and readable."
      ],
      "metadata": {
        "id": "3PkPhUWXiuAN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import json\n",
        "import nltk\n",
        "import time\n",
        "import praw\n",
        "import spacy\n",
        "import gensim\n",
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "from bertopic import BERTopic\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from nltk.corpus import stopwords\n",
        "from gensim.models import Word2Vec\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from sklearn.cluster import KMeans, DBSCAN\n",
        "from collections import Counter, defaultdict\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.decomposition import LatentDirichletAllocation, NMF\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer"
      ],
      "metadata": {
        "id": "MSMTwAnZkjWG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# A. Data Collection"
      ],
      "metadata": {
        "id": "OOlnEPDyk3eX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "CLIENT_ID = \"NxpupKkdJTVW7j-Y0FYhpQ\"\n",
        "CLIENT_SECRET = \"NRoWG7ojanGMXmtOur9j77sGaFApYw\"\n",
        "USER_AGENT = \"MyResearchProject:v1.0 (by /u/Unusual-Fishing8775)\""
      ],
      "metadata": {
        "id": "Qo-jfzqQmA1n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines API credentials (CLIENT_ID, CLIENT_SECRET, USER_AGENT) to connect to Reddit via PRAW. The fetch_reddit_praw function takes a list of subreddits and a total post count (size). It splits the number of posts per subreddit, connects to Reddit, and iterates over the hot posts, combining the title and body into full_text. Each post is stored in a dictionary with source, subreddit, text, and timestamp. Finally, all dictionaries are converted into a pandas DataFrame. Errors during fetching are caught and printed, ensuring the loop continues for remaining subreddits."
      ],
      "metadata": {
        "id": "-Gis3kpCfJ6h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_reddit_praw(subreddits: list[str], size: int = 100):\n",
        "    if not CLIENT_ID or CLIENT_ID == \"YOUR_CLIENT_ID\":\n",
        "        print(\"!!! Reddit API credentials are not set. Please update CLIENT_ID and CLIENT_SECRET.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    print(\"Connecting to Reddit API...\")\n",
        "    reddit = praw.Reddit(\n",
        "        client_id=CLIENT_ID,\n",
        "        client_secret=CLIENT_SECRET,\n",
        "        user_agent=USER_AGENT,\n",
        "    )\n",
        "\n",
        "    out = []\n",
        "    limit_per_subreddit = int(size / len(subreddits)) + 1\n",
        "\n",
        "    print(f\"Fetching {limit_per_subreddit} posts from each of these subreddits: {subreddits}\")\n",
        "\n",
        "    for sub in subreddits:\n",
        "        try:\n",
        "            subreddit = reddit.subreddit(sub)\n",
        "            for submission in subreddit.hot(limit=limit_per_subreddit):\n",
        "                full_text = submission.title + \" \" + submission.selftext\n",
        "                out.append({\n",
        "                    \"source\": \"reddit\",\n",
        "                    \"subreddit\": sub,\n",
        "                    \"text\": full_text.strip(),\n",
        "                    \"created_utc\": submission.created_utc\n",
        "                })\n",
        "        except Exception as e:\n",
        "            print(f\"Could not fetch data for subreddit '{sub}'. Error: {e}\")\n",
        "            continue\n",
        "\n",
        "    df = pd.DataFrame(out)\n",
        "    return df"
      ],
      "metadata": {
        "id": "H41XOSwImLzo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function, collect_sample, sets default subreddits if none are provided and calls fetch_reddit_praw to retrieve posts. It prints the subreddits being queried and the target size, then returns the resulting DataFrame, also reporting the total number of posts collected if the DataFrame is not empty."
      ],
      "metadata": {
        "id": "trZzcgL-fteJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_sample(subreddits=None, reddit_size=100):\n",
        "    if subreddits is None:\n",
        "        subreddits = [\"entrepreneur\", \"startups\", \"solopreneurs\", \"passive_income\"]\n",
        "\n",
        "    print(f\"Collecting posts from subreddits: {subreddits} (target total size={reddit_size})\")\n",
        "    df = fetch_reddit_praw(subreddits, size=reddit_size)\n",
        "\n",
        "    if not df.empty:\n",
        "        print(f\"Collected {len(df)} total items.\")\n",
        "    return df"
      ],
      "metadata": {
        "id": "6QfldTdImMDS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# B. Pre-processing"
      ],
      "metadata": {
        "id": "rfNnCQiRmhR6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The clean_text_basic function preprocesses text by first checking if the input is a string, returning an empty string otherwise. It removes URLs using a regular expression, deletes non-printable or control characters, and finally strips leading and trailing whitespace. This produces cleaner, standardized text suitable for further NLP processing or analysis."
      ],
      "metadata": {
        "id": "PFGwE0cmf0kv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text_basic(text: str):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    t = text\n",
        "    t = re.sub(r\"https?://\\S+|www\\.\\S+\", \" \", t)\n",
        "    t = re.sub(r\"[\\x00-\\x1f\\x7f-\\x9f]\", \" \", t)\n",
        "    t = t.strip()\n",
        "    return t"
      ],
      "metadata": {
        "id": "XHEwv_xEmSOI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The normalize_lower_remove_punct function standardizes text by converting it to lowercase, removing all characters except letters, numbers, and whitespace, and replacing multiple spaces with a single space. Finally, it strips leading and trailing spaces. This produces clean, uniform text ideal for NLP tasks like tokenization, vectorization, or topic modeling."
      ],
      "metadata": {
        "id": "80OyLZhdf20Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_lower_remove_punct(text: str):\n",
        "    t = text.lower()\n",
        "    t = re.sub(r\"[^a-z0-9\\s]\", \" \", t)\n",
        "    t = re.sub(r\"\\s+\", \" \", t).strip()\n",
        "    return t"
      ],
      "metadata": {
        "id": "X0W7HraYmSUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The tokenize_and_lemmatize function converts text into meaningful tokens while reducing words to their base forms. If use_spacy is True and spaCy is available, it loads the English NLP model, removes spaces, punctuation, URLs, and stopwords, and lemmatizes each token. Otherwise, it uses NLTK to tokenize, lowercase, remove stopwords/non-alphanumeric words, and lemmatize. The function returns a cleaned list of tokens suitable for NLP tasks like embeddings, topic modeling, or clustering."
      ],
      "metadata": {
        "id": "v5L3CdZLgCih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_lemmatize(text: str, use_spacy=False):\n",
        "    if use_spacy and _HAS_SPACY:\n",
        "        global _SPACY_NLP\n",
        "        try:\n",
        "            _SPACY_NLP\n",
        "        except NameError:\n",
        "            try:\n",
        "                _SPACY_NLP = spacy.load(\"en_core_web_sm\")\n",
        "            except Exception:\n",
        "                spacy.cli.download(\"en_core_web_sm\")\n",
        "                _SPACY_NLP = spacy.load(\"en_core_web_sm\")\n",
        "        doc = _SPACY_NLP(text)\n",
        "        tokens = []\n",
        "        for tok in doc:\n",
        "            if tok.is_space or tok.is_punct or tok.like_url:\n",
        "                continue\n",
        "            if tok.is_stop:\n",
        "                continue\n",
        "            lemma = tok.lemma_.strip().lower()\n",
        "            if lemma and len(lemma) > 1:\n",
        "                tokens.append(lemma)\n",
        "        return tokens\n",
        "    else:\n",
        "        toks = word_tokenize(text)\n",
        "        tokens = []\n",
        "        for w in toks:\n",
        "            w = w.lower()\n",
        "            if w in STOPWORDS or not re.match(r\"^[a-z0-9]+$\", w):\n",
        "                continue\n",
        "            lemma = LEMMATIZER.lemmatize(w)\n",
        "            tokens.append(lemma)\n",
        "        return tokens"
      ],
      "metadata": {
        "id": "SuLJgIzkmSY5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The preprocess_dataframe function cleans and prepares a DataFrame of text data for NLP. It first copies the DataFrame and ensures the text column is a string. It applies clean_text_basic and normalize_lower_remove_punct to standardize text, then tokenizes and lemmatizes using tokenize_and_lemmatize. Rows with empty tokens are removed, tokens are joined into processed_text, and duplicates are dropped. Finally, it returns the DataFrame with raw, cleaned, tokenized, and processed columns, ready for analysis or modeling."
      ],
      "metadata": {
        "id": "QDFLYAFQgHBa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_dataframe(df: pd.DataFrame, use_spacy=False):\n",
        "    print(\"Starting preprocessing\")\n",
        "    df = df.copy()\n",
        "    df[\"text_raw\"] = df[\"text\"].astype(str)\n",
        "\n",
        "    df[\"clean_basic\"] = df[\"text_raw\"].apply(clean_text_basic)\n",
        "    df[\"clean_norm\"] = df[\"clean_basic\"].apply(normalize_lower_remove_punct)\n",
        "\n",
        "    df[\"tokens\"] = df[\"clean_norm\"].apply(lambda t: tokenize_and_lemmatize(t, use_spacy=use_spacy))\n",
        "    df = df[df[\"tokens\"].apply(len) > 0].reset_index(drop=True)\n",
        "    df[\"processed_text\"] = df[\"tokens\"].apply(lambda tok: \" \".join(tok))\n",
        "\n",
        "    before = len(df)\n",
        "    df = df.drop_duplicates(subset=[\"processed_text\"]).reset_index(drop=True)\n",
        "    after = len(df)\n",
        "    print(f\"Preprocessing done: {before} -> {after} after deduplication\")\n",
        "    return df[[\"source\", \"subreddit\", \"text_raw\", \"clean_basic\", \"clean_norm\", \"tokens\", \"processed_text\", \"created_utc\"]]"
      ],
      "metadata": {
        "id": "CM9pETIZmSdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# C. Vectorization"
      ],
      "metadata": {
        "id": "3ZQtLfGjmmmo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vectorize_tfidf function converts a list of preprocessed texts into a TF-IDF matrix, capturing the importance of words relative to the corpus. It initializes a TfidfVectorizer with a maximum number of features (max_features) and an n-gram range (ngram). The fit_transform method learns the vocabulary and transforms the texts into a sparse matrix X, which is returned along with the vectorizer for later use."
      ],
      "metadata": {
        "id": "PpsIT4zggiyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_tfidf(processed_texts, max_features=2000, ngram=(1, 2)):\n",
        "    vec = TfidfVectorizer(max_features=max_features, ngram_range=ngram)\n",
        "    X = vec.fit_transform(processed_texts)\n",
        "    print(f\"TF-IDF matrix shape: {X.shape}\")\n",
        "    return X, vec"
      ],
      "metadata": {
        "id": "wA6_GV_dmr9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vectorize_count function transforms preprocessed texts into a document-term matrix using raw word counts. It creates a CountVectorizer with a specified maximum number of features (max_features) to limit vocabulary size. The fit_transform method learns the vocabulary from the texts and converts them into a sparse matrix X, which, along with the vectorizer, is returned for further analysis or modeling."
      ],
      "metadata": {
        "id": "uCHTPxWygm0D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_count(processed_texts, max_features=2000):\n",
        "    vec = CountVectorizer(max_features=max_features)\n",
        "    X = vec.fit_transform(processed_texts)\n",
        "    print(f\"Count matrix shape: {X.shape}\")\n",
        "    return X, vec"
      ],
      "metadata": {
        "id": "XI_40z-EmsAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The vectorize_sbert function generates semantic embeddings for a list of preprocessed texts using Sentence-BERT. It first checks if SBERT is available, then loads the specified model (model_name). The encode method transforms texts into dense numerical vectors (emb) representing their semantic meaning. It prints the embedding matrix shape and returns both the embeddings and the model for downstream tasks like clustering or similarity analysis."
      ],
      "metadata": {
        "id": "HsY55TNygowC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize_sbert(processed_texts, model_name=\"all-MiniLM-L6-v2\"):\n",
        "    if not _HAS_SBERT:\n",
        "        raise RuntimeError(\"sentence-transformers not installed; install to use SBERT embeddings.\")\n",
        "    model = SentenceTransformer(model_name)\n",
        "    emb = model.encode(processed_texts, show_progress_bar=True, convert_to_numpy=True)\n",
        "    print(f\"SBERT embeddings shape: {emb.shape}\")\n",
        "    return emb, model"
      ],
      "metadata": {
        "id": "H8GiHR_wmsDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The train_word2vec function trains a Word2Vec model on tokenized text data (token_lists) using Gensim. It checks for Gensim availability, then trains embeddings with specified vector_size, min_count, and epochs. For each document, it averages the vectors of its tokens to create a document-level embedding. If a document has no known tokens, a zero vector is used. The function returns the array of document embeddings and the trained Word2Vec model."
      ],
      "metadata": {
        "id": "kZ35EgMwgq0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_word2vec(token_lists, vector_size=100, min_count=2):\n",
        "    if not _HAS_GENSIM:\n",
        "        raise RuntimeError(\"gensim not installed; install to use Word2Vec.\")\n",
        "    w2v = Word2Vec(sentences=token_lists, vector_size=vector_size, min_count=min_count, workers=2, epochs=10)\n",
        "    print(\"Word2Vec trained\")\n",
        "    doc_vecs = []\n",
        "    for toks in token_lists:\n",
        "        vecs = [w2v.wv[w] for w in toks if w in w2v.wv]\n",
        "        if vecs:\n",
        "            doc_vecs.append(np.mean(vecs, axis=0))\n",
        "        else:\n",
        "            doc_vecs.append(np.zeros(vector_size))\n",
        "    return np.array(doc_vecs), w2v"
      ],
      "metadata": {
        "id": "MQRvMPeWmsF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# D. Analytics: Topic Modeling & Clustering"
      ],
      "metadata": {
        "id": "oAqg83PCm3OP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### LDA"
      ],
      "metadata": {
        "id": "i6vDhuPkm-Xg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These two functions perform topic modeling on text data using different algorithms.\n",
        "\n",
        "run_lda takes a document-term count_matrix and a CountVectorizer. It fits a Latent Dirichlet Allocation (LDA) model with n_topics. For each topic, it identifies the top n_top_words based on component weights and returns both the trained LDA model and a dictionary of topics with their top words.\n",
        "\n",
        "run_nmf uses a tfidf_matrix and TfidfVectorizer to fit a Non-negative Matrix Factorization (NMF) model. Similarly, it extracts the top words for each topic and returns the NMF model along with a dictionary mapping topics to their top words. Both methods help uncover latent themes in the text corpus."
      ],
      "metadata": {
        "id": "euB-b4jRhKIj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_lda(count_matrix, count_vectorizer, n_topics=6, n_top_words=10):\n",
        "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
        "    lda.fit(count_matrix)\n",
        "    terms = count_vectorizer.get_feature_names_out()\n",
        "    topics = {}\n",
        "    for i, comp in enumerate(lda.components_):\n",
        "        top_idx = comp.argsort()[:-n_top_words - 1:-1]\n",
        "        topics[f\"LDA_{i}\"] = [terms[idx] for idx in top_idx]\n",
        "    return lda, topics\n",
        "\n",
        "\n",
        "def run_nmf(tfidf_matrix, tfidf_vectorizer, n_topics=6, n_top_words=10):\n",
        "    nmf = NMF(n_components=n_topics, random_state=42)\n",
        "    nmf.fit(tfidf_matrix)\n",
        "    terms = tfidf_vectorizer.get_feature_names_out()\n",
        "    topics = {}\n",
        "    for i, comp in enumerate(nmf.components_):\n",
        "        top_idx = comp.argsort()[:-n_top_words - 1:-1]\n",
        "        topics[f\"NMF_{i}\"] = [terms[idx] for idx in top_idx]\n",
        "    return nmf, topics"
      ],
      "metadata": {
        "id": "ob4Q_t0anEdB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERTopic"
      ],
      "metadata": {
        "id": "5lWqoyBYnHgh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The run_bertopic_if_available function applies BERTopic topic modeling if the library is installed. It first checks the _HAS_BERTOPIC flag; if not available, it prints a message and returns None. If available, it initializes a BERTopic model, fits it on the processed_texts, and returns the trained model along with the predicted topic labels (topics) and their probabilities (probs) for each document."
      ],
      "metadata": {
        "id": "fnR0jP9dhOgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_bertopic_if_available(processed_texts):\n",
        "    if not _HAS_BERTOPIC:\n",
        "        print(\"BERTopic not installed — skipping BERTopic.\")\n",
        "        return None\n",
        "    model = BERTopic(verbose=False)\n",
        "    topics, probs = model.fit_transform(processed_texts)\n",
        "    return model, topics, probs"
      ],
      "metadata": {
        "id": "sAFVKGWnnEfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### K-Means"
      ],
      "metadata": {
        "id": "CH3pGAztnMYR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The cluster_kmeans function performs KMeans clustering on a given feature_matrix. It initializes a KMeans model with a specified number of clusters (n_clusters) and a fixed random state for reproducibility. The fit_predict method assigns each data point to a cluster and returns both the cluster labels and the trained KMeans model for further analysis or visualization."
      ],
      "metadata": {
        "id": "66iG2DM9hRkD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cluster_kmeans(feature_matrix, n_clusters=6):\n",
        "    print(\"Running KMeans\")\n",
        "    km = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "    labels = km.fit_predict(feature_matrix)\n",
        "    return labels, km"
      ],
      "metadata": {
        "id": "xn0VD0Z8nPCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training Classification Models"
      ],
      "metadata": {
        "id": "ysln2OEEnWkw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The train_classifier function trains a Logistic Regression model to predict cluster or topic labels. It first splits the feature matrix (X_features) and labels (y_labels) into training and testing sets (80/20). The model is trained on the training data, then used to predict the test set. It prints the classifier’s accuracy and a detailed classification report, returning the trained model and test results for evaluation."
      ],
      "metadata": {
        "id": "yjjumDZWhYML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_classifier(X_features, y_labels):\n",
        "    print(\"Training classifier (Logistic Regression) to predict cluster labels\")\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_features, y_labels, test_size=0.2, random_state=42)\n",
        "    clf = LogisticRegression(max_iter=1000)\n",
        "    clf.fit(X_train, y_train)\n",
        "    preds = clf.predict(X_test)\n",
        "    print(\"Classifier accuracy:\", accuracy_score(y_test, preds))\n",
        "    print(classification_report(y_test, preds))\n",
        "    return clf, (X_test, y_test, preds)"
      ],
      "metadata": {
        "id": "xbu6yxyinfhA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trend tracking (simple)"
      ],
      "metadata": {
        "id": "nkcwZ3jynhiC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The infer_trends_over_time function analyzes word frequency trends in a processed text DataFrame over time. It checks if the time_col exists; if not, it returns the 20 most common words globally. Otherwise, it converts timestamps to periods (freq, e.g., monthly), groups the data by period, and counts word occurrences in each group. The function returns a dictionary mapping each time period to its top 20 most frequent words, helping identify evolving topics or trends."
      ],
      "metadata": {
        "id": "4hIs0NjWhe4b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def infer_trends_over_time(df_processed, text_col=\"processed_text\", time_col=\"created_utc\", freq=\"M\"):\n",
        "    if time_col not in df_processed.columns or df_processed[time_col].isnull().all():\n",
        "        tokens = \" \".join(df_processed[text_col].tolist()).split()\n",
        "        return {\"global\": Counter(tokens).most_common(20)}\n",
        "\n",
        "    df = df_processed.copy()\n",
        "    df[time_col] = pd.to_datetime(df[time_col], unit=\"s\", errors=\"coerce\")\n",
        "    df = df.dropna(subset=[time_col])\n",
        "    df[\"period\"] = df[time_col].dt.to_period(freq)\n",
        "\n",
        "    period_top = {}\n",
        "    for period, group in df.groupby(\"period\"):\n",
        "        toks = \" \".join(group[text_col].tolist()).split()\n",
        "        period_top[str(period)] = Counter(toks).most_common(20)\n",
        "    return period_top"
      ],
      "metadata": {
        "id": "PoOX3qgHnfjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Running Simualtion"
      ],
      "metadata": {
        "id": "14muQaRZnt9o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A. Data Collection"
      ],
      "metadata": {
        "id": "Ma-aoXKZqlTL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "subreddits=None → If no subreddits are specified, the code will use a predefined list.\n",
        "\n",
        "reddit_size=120 → Target total number of posts to fetch from all subreddits.\n",
        "\n",
        "use_spacy=False → Determines whether to use spaCy for tokenization and lemmatization.\n",
        "\n",
        "vector_method=\"tfidf\" → Text vectorization approach (tfidf, count, sbert, or word2vec).\n",
        "\n",
        "n_topics=6 → Number of topics for topic modeling algorithms (LDA, NMF, BERTopic).\n",
        "\n",
        "clustering_method=\"kmeans\" → Clustering algorithm to use on feature vectors."
      ],
      "metadata": {
        "id": "GLzeJ8WmhoRD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "subreddits=None\n",
        "reddit_size=120\n",
        "use_spacy=False\n",
        "vector_method=\"tfidf\"\n",
        "n_topics=6\n",
        "clustering_method=\"kmeans\""
      ],
      "metadata": {
        "id": "gKJ1V9wAnfl6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code calls collect_sample to fetch Reddit posts using the specified subreddits and reddit_size. It stores the result in df_raw. If no data is returned (i.e., the DataFrame is empty or has zero rows), it prints a message indicating no data was collected and stops further execution, preventing errors in downstream processing."
      ],
      "metadata": {
        "id": "GFwJYRDYhrnj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_raw = collect_sample(subreddits=subreddits, reddit_size=reddit_size)\n",
        "if df_raw.empty or len(df_raw) == 0:\n",
        "  print(\"[main] No data collected; exiting\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wAANeRp6nz6I",
        "outputId": "8de99590-8be5-47a2-fa96-2fceded796b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting posts from subreddits: ['entrepreneur', 'startups', 'solopreneurs', 'passive_income'] (target total size=120)\n",
            "Connecting to Reddit API...\n",
            "Fetching 31 posts from each of these subreddits: ['entrepreneur', 'startups', 'solopreneurs', 'passive_income']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n",
            "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
            "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
            "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collected 102 total items.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code filters the collected Reddit posts for relevance using a keyword-based approach. A list of business-related keywords is defined and joined into a single regular expression pattern. The str.contains method checks each post’s text for these keywords (case-insensitive). Posts matching any keyword are retained in df_filtered, and the number of filtered posts is printed, ensuring only relevant content is analyzed further."
      ],
      "metadata": {
        "id": "M1rl83rhhvD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"\\nFiltering {len(df_raw)} collected posts for relevance...\")\n",
        "keywords = [\n",
        "  'income', 'saas', 'business', 'startup', 'freelance', 'hustle',\n",
        "  'e-commerce', 'marketing', 'revenue', 'profit', 'customer', 'growth',\n",
        "  'side hustle', 'mrr', 'arr', 'funding', 'validate'\n",
        "]\n",
        "pattern = '|'.join(keywords)\n",
        "mask = df_raw['text'].str.contains(pattern, case=False, na=False)\n",
        "df_filtered = df_raw[mask]\n",
        "\n",
        "print(f\"Retained {len(df_filtered)} posts after keyword filtering.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NsQW7WCfnz8s",
        "outputId": "de8ce2b2-5814-44c7-edc0-679b3a9e6ba9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Filtering 102 collected posts for relevance...\n",
            "Retained 77 posts after keyword filtering.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code visualizes the distribution of posts across subreddits. value_counts() counts how many posts come from each subreddit, and reset_index() converts it into a DataFrame with columns subreddit and count. sns.barplot from Seaborn then creates a bar chart, using the “viridis” color palette, showing the number of posts per subreddit for quick visual analysis."
      ],
      "metadata": {
        "id": "Uz9H5WVwh9Lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "counts = df_filtered['subreddit'].value_counts().reset_index()\n",
        "sns.barplot(data=counts, x='subreddit', y='count', palette=\"viridis\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "DGxQSePsoswJ",
        "outputId": "ba88f8fc-c20f-4819-83c2-0107da43d241"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Axes: xlabel='subreddit', ylabel='count'>"
            ]
          },
          "metadata": {},
          "execution_count": 22
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## B. Pre-processing"
      ],
      "metadata": {
        "id": "NZAyopmDqp9b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line applies the preprocess_dataframe function to a copy of the filtered DataFrame df_filtered. It cleans the text by applying basic cleaning, normalization, tokenization, and lemmatization (using spaCy if use_spacy=True). The resulting DataFrame df_proc includes additional columns like tokens and processed_text, providing structured, cleaned text ready for vectorization, topic modeling, or clustering."
      ],
      "metadata": {
        "id": "gH84MjyliBV8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_proc = preprocess_dataframe(df_filtered.copy(), use_spacy=use_spacy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oprcB6Cdnz_Z",
        "outputId": "0df07fc4-ca8b-4eab-a692-6a8e6a9f3bf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting preprocessing\n",
            "Preprocessing done: 77 -> 77 after deduplication\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## C. Vectorization"
      ],
      "metadata": {
        "id": "MJLVddZRqv_j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code converts the preprocessed texts into numerical feature representations for modeling, based on the chosen vector_method.\n",
        "\n",
        "For \"tfidf\", it creates a TF-IDF matrix (tfidf_X) and a count matrix (count_X), using TF-IDF as the main feature matrix.\n",
        "\n",
        "For \"sbert\", it uses SBERT embeddings if available; otherwise, it falls back to TF-IDF.\n",
        "\n",
        "For \"word2vec\", it trains a Word2Vec model to generate document embeddings, defaulting to TF-IDF if Gensim is unavailable.\n",
        "\n",
        "The resulting feature_matrix is used for clustering or classification, while TF-IDF and count matrices are always generated for topic modeling."
      ],
      "metadata": {
        "id": "uUSb5H59iGuU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "processed_texts = df_proc[\"processed_text\"].tolist()\n",
        "tokens_lists = df_proc[\"tokens\"].tolist()\n",
        "\n",
        "if vector_method == \"tfidf\":\n",
        "    tfidf_X, tfidf_vec = vectorize_tfidf(processed_texts, max_features=3000)\n",
        "    count_X, count_vec = vectorize_count(processed_texts, max_features=3000)\n",
        "    feature_matrix = tfidf_X\n",
        "elif vector_method == \"sbert\":\n",
        "    if not _HAS_SBERT:\n",
        "        print(\"SBERT not available, falling back to TF-IDF\")\n",
        "        tfidf_X, tfidf_vec = vectorize_tfidf(processed_texts)\n",
        "        count_X, count_vec = vectorize_count(processed_texts)\n",
        "        feature_matrix = tfidf_X\n",
        "    else:\n",
        "        emb, sbert_model = vectorize_sbert(processed_texts)\n",
        "        feature_matrix = emb\n",
        "        count_X, count_vec = vectorize_count(processed_texts)\n",
        "        tfidf_X, tfidf_vec = vectorize_tfidf(processed_texts)\n",
        "elif vector_method == \"word2vec\":\n",
        "    if not _HAS_GENSIM:\n",
        "        print(\"gensim not available, falling back to TF-IDF\")\n",
        "        tfidf_X, tfidf_vec = vectorize_tfidf(processed_texts)\n",
        "        count_X, count_vec = vectorize_count(processed_texts)\n",
        "        feature_matrix = tfidf_X\n",
        "    else:\n",
        "        doc_vecs, w2v_model = train_word2vec(tokens_lists)\n",
        "        feature_matrix = doc_vecs\n",
        "        count_X, count_vec = vectorize_count(processed_texts)\n",
        "        tfidf_X, tfidf_vec = vectorize_tfidf(processed_texts)\n",
        "else:\n",
        "    raise ValueError(\"Unknown vector_method\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4-r41C6n0B8",
        "outputId": "531d1ee5-41c3-487c-f2a6-58482cab63d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TF-IDF matrix shape: (77, 3000)\n",
            "Count matrix shape: (77, 2517)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## D. Analytics: Topic Modeling & Clustering"
      ],
      "metadata": {
        "id": "Doz5IIygq3ay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  LDA"
      ],
      "metadata": {
        "id": "JcyVvSiKq6lC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code performs LDA topic modeling on the count-based document-term matrix (count_X) using the count_vec for feature names and n_topics topics. The run_lda function returns the trained LDA model (lda_model) and a dictionary of topics (lda_topics). The loop then prints each topic label along with its top words, providing insight into the main themes present in the text corpus."
      ],
      "metadata": {
        "id": "_uYdARGAiLRt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lda_model, lda_topics = run_lda(count_X, count_vec, n_topics=n_topics)\n",
        "print(\"\\nLDA topics (top words):\")\n",
        "for k, v in lda_topics.items():\n",
        "    print(k, \":\", \", \".join(v))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6faNoZ9kn0Ej",
        "outputId": "701e78f1-950f-475c-fb67-32d16c0e4b15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "LDA topics (top words):\n",
            "LDA_0 : like, idea, people, income, business, work, project, guy, month, make\n",
            "LDA_1 : month, business, make, work, like, time, site, money, want, lead\n",
            "LDA_2 : business, shirt, would, google, work, online, know, one, like, ad\n",
            "LDA_3 : business, got, like, would, one, time, 50, promote, feel, even\n",
            "LDA_4 : product, build, video, saas, month, idea, course, time, user, revenue\n",
            "LDA_5 : income, product, passive, startup, using, business, market, company, share, get\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code applies NMF topic modeling on the TF-IDF feature matrix (tfidf_X) using the corresponding vectorizer (tfidf_vec) and n_topics topics. The run_nmf function returns the trained NMF model (nmf_model) and a dictionary of topics (nmf_topics). The loop iterates over each topic, printing its label and top words, revealing dominant themes and patterns in the text data."
      ],
      "metadata": {
        "id": "eRkOOKDUiQJs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nmf_model, nmf_topics = run_nmf(tfidf_X, tfidf_vec, n_topics=n_topics)\n",
        "print(\"\\nNMF topics (top words):\")\n",
        "for k, v in nmf_topics.items():\n",
        "    print(k, \":\", \", \".join(v))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjZl_EanoNe5",
        "outputId": "01ee823b-8e8b-4bbc-a42f-1820a1081a7b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "NMF topics (top words):\n",
            "NMF_0 : work, want, shirt, money, make, know, something, business, year, month\n",
            "NMF_1 : start online, online business, start, online, business, money, resource, today, would, creation something\n",
            "NMF_2 : passive, income, passive income, people, coupon, tiktok, way, automation, free, affiliate\n",
            "NMF_3 : product, first, video, user, mvp, month, build, saas, problem, course\n",
            "NMF_4 : startup, investor, roast, maga, please, founder, promote, requirement, ip, idea\n",
            "NMF_5 : like, feel, feel like, next, would, actually, got, feeling, business, guy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### BERTopic (transformer-based)"
      ],
      "metadata": {
        "id": "0Uey03Y_rHSr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code attempts to run BERTopic if the library is installed (_HAS_BERTOPIC). It calls run_bertopic_if_available on processed_texts, returning the model, topic labels, and probabilities. The results are stored in bert_results. It then prints a sample of the top five topics using get_topic_info(). Any errors during execution are caught and printed, ensuring the script continues without crashing."
      ],
      "metadata": {
        "id": "JZBIUCO5iTA8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bert_results = None\n",
        "if _HAS_BERTOPIC:\n",
        "    try:\n",
        "        bert_model, bert_topics, bert_probs = run_bertopic_if_available(processed_texts)\n",
        "        bert_results = {\"model\": bert_model, \"topics\": bert_topics, \"probs\": bert_probs}\n",
        "        print(\"BERTopic sample topics info:\")\n",
        "        print(bert_model.get_topic_info().head(5))\n",
        "    except Exception as e:\n",
        "        print(\"BERTopic error:\", e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYo0femRoNho",
        "outputId": "a4711483-e889-41ee-ead1-7c5caabd3d61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BERTopic sample topics info:\n",
            "   Topic  Count                              Name  \\\n",
            "0     -1     24  -1_product_work_business_startup   \n",
            "1      0     28        0_month_income_like_people   \n",
            "2      1     25      1_business_feel_like_founder   \n",
            "\n",
            "                                      Representation  \\\n",
            "0  [product, work, business, startup, saas, video...   \n",
            "1  [month, income, like, people, business, passiv...   \n",
            "2  [business, feel, like, founder, promote, start...   \n",
            "\n",
            "                                 Representative_Docs  \n",
            "0  [share startup quarterly post share startup q4...  \n",
            "1  [best passive income idea sept 2025 september ...  \n",
            "2  [feeling burnt kickstarter sure go b2c b2b pro...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Clustering (K-Means & DBSCAN)"
      ],
      "metadata": {
        "id": "6GcUAypQrMm6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code performs KMeans clustering on the feature_matrix using n_topics as the number of clusters. The cluster_kmeans function returns cluster labels and the trained model. The cluster labels are added to df_proc in a new cluster column. Finally, value_counts() prints the number of posts assigned to each cluster, showing the distribution of documents across clusters."
      ],
      "metadata": {
        "id": "R5EmqTr5iaSs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "labels, cluster_model = cluster_kmeans(feature_matrix, n_clusters=n_topics)\n",
        "df_proc[\"cluster\"] = labels\n",
        "print(\"Cluster sizes:\")\n",
        "print(df_proc[\"cluster\"].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLwjnEwpoNj9",
        "outputId": "984f4670-fcea-4c78-aa1d-955b3a93ec77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running KMeans\n",
            "Cluster sizes:\n",
            "cluster\n",
            "3    36\n",
            "5    13\n",
            "1    10\n",
            "4     8\n",
            "0     7\n",
            "2     3\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line trains a Logistic Regression classifier to predict the KMeans cluster labels using the feature_matrix as input features and the cluster column from df_proc as labels. The train_classifier function splits the data into training and test sets, fits the model, evaluates it with accuracy and a classification report, and returns both the trained classifier (clf) and the test evaluation results (clf_eval)."
      ],
      "metadata": {
        "id": "m5qBckFzied0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clf, clf_eval = train_classifier(feature_matrix, df_proc[\"cluster\"].values)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G1nfH1mGoNmV",
        "outputId": "ea09bd21-d7ea-4508-994d-a4e53772cbea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training classifier (Logistic Regression) to predict cluster labels\n",
            "Classifier accuracy: 0.3125\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00         1\n",
            "           1       0.00      0.00      0.00         3\n",
            "           2       0.00      0.00      0.00         1\n",
            "           3       0.31      1.00      0.48         5\n",
            "           5       0.00      0.00      0.00         6\n",
            "\n",
            "    accuracy                           0.31        16\n",
            "   macro avg       0.06      0.20      0.10        16\n",
            "weighted avg       0.10      0.31      0.15        16\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Trend tracking"
      ],
      "metadata": {
        "id": "Jslxqbexram2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code analyzes word frequency trends over time using the infer_trends_over_time function. It groups the processed texts (processed_text) by month (freq=\"M\") based on the created_utc timestamps. The function returns the top words per period. The loop then prints the first three periods with their top eight most frequent words, highlighting evolving topics or trends in the dataset.\n"
      ],
      "metadata": {
        "id": "_KRXzF4ZihOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trends = infer_trends_over_time(df_proc, text_col=\"processed_text\", time_col=\"created_utc\", freq=\"M\")\n",
        "print(\"\\n[Trend] sample (first periods):\")\n",
        "for period, top in list(trends.items())[:3]:\n",
        "    print(period, \"->\", top[:8])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VUNl2YydoNon",
        "outputId": "c8c413d2-df30-45e4-82bc-796501b3305f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Trend] sample (first periods):\n",
            "2016-01 -> [('beard', 9), ('started', 7), ('year', 6), ('making', 6), ('actually', 6), ('business', 6), ('also', 5), ('around', 5)]\n",
            "2018-08 -> [('digital', 2), ('buzzmode', 1), ('distinctive', 1), ('service', 1), ('business', 1), ('enhancement', 1), ('support', 1), ('entity', 1)]\n",
            "2018-11 -> [('bookkeeping', 2), ('software', 2), ('solopreneurs', 2), ('use', 2), ('pay', 2), ('hi', 1), ('launched', 1), ('easybook', 1)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_proc.head(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 432
        },
        "id": "igj5bRjpoNq2",
        "outputId": "efddf512-d602-4266-cfe7-8a2ed852706c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   source     subreddit                                           text_raw  \\\n",
              "0  reddit  entrepreneur  Learned the hard way: not EVERY accountant get...   \n",
              "1  reddit  entrepreneur  Roast my startup Hi, I am the Founder of Podza...   \n",
              "2  reddit  entrepreneur  I tested a business idea with just a landing p...   \n",
              "3  reddit  entrepreneur  How do you guys actually switch off? I’ve been...   \n",
              "4  reddit  entrepreneur  Just negotiated scope of work with a $25 milli...   \n",
              "\n",
              "                                         clean_basic  \\\n",
              "0  Learned the hard way: not EVERY accountant get...   \n",
              "1  Roast my startup Hi, I am the Founder of Podza...   \n",
              "2  I tested a business idea with just a landing p...   \n",
              "3  How do you guys actually switch off? I’ve been...   \n",
              "4  Just negotiated scope of work with a $25 milli...   \n",
              "\n",
              "                                          clean_norm  \\\n",
              "0  learned the hard way not every accountant gets...   \n",
              "1  roast my startup hi i am the founder of podzay...   \n",
              "2  i tested a business idea with just a landing p...   \n",
              "3  how do you guys actually switch off i ve been ...   \n",
              "4  just negotiated scope of work with a 25 millio...   \n",
              "\n",
              "                                              tokens  \\\n",
              "0  [learned, hard, way, every, accountant, get, e...   \n",
              "1  [roast, startup, hi, founder, podzay, tool, co...   \n",
              "2  [tested, business, idea, landing, page, genera...   \n",
              "3  [guy, actually, switch, running, little, busin...   \n",
              "4  [negotiated, scope, work, 25, million, non, pr...   \n",
              "\n",
              "                                      processed_text   created_utc  cluster  \n",
              "0  learned hard way every accountant get e commer...  1.758187e+09        3  \n",
              "1  roast startup hi founder podzay tool connects ...  1.758179e+09        4  \n",
              "2  tested business idea landing page generating 4...  1.758112e+09        3  \n",
              "3  guy actually switch running little business ho...  1.758113e+09        3  \n",
              "4  negotiated scope work 25 million non profit ne...  1.758161e+09        2  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-a561afad-22f8-4d0b-b4d7-ff0929b2113e\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>source</th>\n",
              "      <th>subreddit</th>\n",
              "      <th>text_raw</th>\n",
              "      <th>clean_basic</th>\n",
              "      <th>clean_norm</th>\n",
              "      <th>tokens</th>\n",
              "      <th>processed_text</th>\n",
              "      <th>created_utc</th>\n",
              "      <th>cluster</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>reddit</td>\n",
              "      <td>entrepreneur</td>\n",
              "      <td>Learned the hard way: not EVERY accountant get...</td>\n",
              "      <td>Learned the hard way: not EVERY accountant get...</td>\n",
              "      <td>learned the hard way not every accountant gets...</td>\n",
              "      <td>[learned, hard, way, every, accountant, get, e...</td>\n",
              "      <td>learned hard way every accountant get e commer...</td>\n",
              "      <td>1.758187e+09</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>reddit</td>\n",
              "      <td>entrepreneur</td>\n",
              "      <td>Roast my startup Hi, I am the Founder of Podza...</td>\n",
              "      <td>Roast my startup Hi, I am the Founder of Podza...</td>\n",
              "      <td>roast my startup hi i am the founder of podzay...</td>\n",
              "      <td>[roast, startup, hi, founder, podzay, tool, co...</td>\n",
              "      <td>roast startup hi founder podzay tool connects ...</td>\n",
              "      <td>1.758179e+09</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>reddit</td>\n",
              "      <td>entrepreneur</td>\n",
              "      <td>I tested a business idea with just a landing p...</td>\n",
              "      <td>I tested a business idea with just a landing p...</td>\n",
              "      <td>i tested a business idea with just a landing p...</td>\n",
              "      <td>[tested, business, idea, landing, page, genera...</td>\n",
              "      <td>tested business idea landing page generating 4...</td>\n",
              "      <td>1.758112e+09</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>reddit</td>\n",
              "      <td>entrepreneur</td>\n",
              "      <td>How do you guys actually switch off? I’ve been...</td>\n",
              "      <td>How do you guys actually switch off? I’ve been...</td>\n",
              "      <td>how do you guys actually switch off i ve been ...</td>\n",
              "      <td>[guy, actually, switch, running, little, busin...</td>\n",
              "      <td>guy actually switch running little business ho...</td>\n",
              "      <td>1.758113e+09</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>reddit</td>\n",
              "      <td>entrepreneur</td>\n",
              "      <td>Just negotiated scope of work with a $25 milli...</td>\n",
              "      <td>Just negotiated scope of work with a $25 milli...</td>\n",
              "      <td>just negotiated scope of work with a 25 millio...</td>\n",
              "      <td>[negotiated, scope, work, 25, million, non, pr...</td>\n",
              "      <td>negotiated scope work 25 million non profit ne...</td>\n",
              "      <td>1.758161e+09</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a561afad-22f8-4d0b-b4d7-ff0929b2113e')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-a561afad-22f8-4d0b-b4d7-ff0929b2113e button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-a561afad-22f8-4d0b-b4d7-ff0929b2113e');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-18b58bd7-43db-4865-a152-f47907ccd4ed\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-18b58bd7-43db-4865-a152-f47907ccd4ed')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-18b58bd7-43db-4865-a152-f47907ccd4ed button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_proc",
              "summary": "{\n  \"name\": \"df_proc\",\n  \"rows\": 77,\n  \"fields\": [\n    {\n      \"column\": \"source\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"reddit\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"subreddit\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"startups\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text_raw\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 77,\n        \"samples\": [\n          \"Just negotiated scope of work with a $25 million non-profit Just negotiated scope of work with a $25 million non-profit.  I know I've been underpaid building their database system for 2 years for $75K (I've had other projects going too so my annual income this year is about $200K of revenue) and have really low overhead where I live and wasn't carrying any employees when I started (one now) and I've been really generous with scope creep but I finally hit the wall and told them if they don't limit the scope the project will fail.  The document was a materpiece.  I sent it straight to the CEO over the objections of the project liason.  They agreed.  I'll finish up in  month.  This will save me about 4 more months of work.  Then I'm going to take a break.  You have to stick up for yourself.  Generosity is not appreciated it becomes expected.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_basic\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 77,\n        \"samples\": [\n          \"Just negotiated scope of work with a $25 million non-profit Just negotiated scope of work with a $25 million non-profit.  I know I've been underpaid building their database system for 2 years for $75K (I've had other projects going too so my annual income this year is about $200K of revenue) and have really low overhead where I live and wasn't carrying any employees when I started (one now) and I've been really generous with scope creep but I finally hit the wall and told them if they don't limit the scope the project will fail.  The document was a materpiece.  I sent it straight to the CEO over the objections of the project liason.  They agreed.  I'll finish up in  month.  This will save me about 4 more months of work.  Then I'm going to take a break.  You have to stick up for yourself.  Generosity is not appreciated it becomes expected.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"clean_norm\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 77,\n        \"samples\": [\n          \"just negotiated scope of work with a 25 million non profit just negotiated scope of work with a 25 million non profit i know i ve been underpaid building their database system for 2 years for 75k i ve had other projects going too so my annual income this year is about 200k of revenue and have really low overhead where i live and wasn t carrying any employees when i started one now and i ve been really generous with scope creep but i finally hit the wall and told them if they don t limit the scope the project will fail the document was a materpiece i sent it straight to the ceo over the objections of the project liason they agreed i ll finish up in month this will save me about 4 more months of work then i m going to take a break you have to stick up for yourself generosity is not appreciated it becomes expected\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"tokens\",\n      \"properties\": {\n        \"dtype\": \"object\",\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"processed_text\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 77,\n        \"samples\": [\n          \"negotiated scope work 25 million non profit negotiated scope work 25 million non profit know underpaid building database system 2 year 75k project going annual income year 200k revenue really low overhead live carrying employee started one really generous scope creep finally hit wall told limit scope project fail document materpiece sent straight ceo objection project liason agreed finish month save 4 month work going take break stick generosity appreciated becomes expected\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"created_utc\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 62535218.02556203,\n        \"min\": 1453244934.0,\n        \"max\": 1758198058.0,\n        \"num_unique_values\": 77,\n        \"samples\": [\n          1758160831.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"cluster\",\n      \"properties\": {\n        \"dtype\": \"int32\",\n        \"num_unique_values\": 6,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    }
  ]
}